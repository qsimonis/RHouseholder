# This code provides artificial data for doing inference in the sparse CCA problem

require(rstiefel) # This will allow the easy generation of random rectangular orthogonal matrices via haar measure over the space of orthogonal matrices

require(mvtnorm) # This will be used for generating multivariate normal observations for our artificial datasets

# For the shared variation, I will generate the loading matrix via it's eigenvalue/svd decomposition
# by generating eigenvalues from the order statistics of a distribution with high mass near 0. This can
# be fulfilled from a gamma(1,2), gamma(2,2), or something similar.

# Generating the eigenvalues:

eigenvalue.simulator <- function(dims, alpha, beta){
  eigenvalues.simulated <- sort(-1/sort(-rgamma(dims, shape = alpha, rate = beta)), decreasing = T)
  return(eigenvalues.simulated)
}

# Note that eigenvalue.simulator will not necessarily produce a strong eigengap, instead, we will use the following function to create a produce a larger eigengap in our matrix.

# This function will return eigenvalues in the same way as eigenvalue.simulator, but will additionally
# push eigenvalues to a small value with probability "percent".

# What exactly will this small value be? It will be some number between 0 and the smallest eigenvalue
# generated. This will be done by multiplying our vector of eigenvalues with a vector generated by the
# following function:

# There are two inputs to this function:

# eigenvalue.list - a list of one of more vectors of eigenvalues sorted highest to lowest.
# percent - percent of eigenvalues to be shrunk towards 0.


# I think it would make more sense if eigenvalue.shrinker pushed only the smaller eigenvalues towards 0 to induce a more natural eigengap.

eigenvalue.simulator.sparse <- function(eigenvalues,percent){
  eigen.min <- min(eigenvalues)
  number.to.shrink <- floor(percent*length(eigenvalues))
  eigenvalues.shrunk <- eigenvalues
  eigenvalues.shrunk[(length(eigenvalues) - number.to.shrink): length(eigenvalues)] <- eigenvalues.shrunk[(length(eigenvalues) - number.to.shrink): length(eigenvalues)]*runif(1, min = 0, min(eigenvalues)/10)
  return(eigenvalues.shrunk)
}


# This will generate the final representation of the shared loading matrix for Bayesian CCA

shared.covariance.generator.eigen <- function(eigenvalues){
  Generated.orthogonal.matrix <-  rstiefel::rustiefel(m = length(eigenvalues), R = length(eigenvalues))
  Generated.covariance <- Generated.orthogonal.matrix%*%diag(eigenvalues)%*%t(Generated.orthogonal.matrix)
  return(Generated.covariance)
}



# We also need a way to generate the loading matrix of our data. I will propose generating our
# loading matrix with normal entries (with different covariance for each column), where the views
# will be generated with a bernoulli.


view.specific.matrix.generator <- function(view1.dim, view2.dim, data1.dim, data2.dim,
                                           variance.params){
  generated.matrix <- matrix(0, ncol = (view1.dim + view2.dim), nrow = (data1.dim + data2.dim))
  column.variances <- rgamma(view1.dim + view2.dim, shape = variance.params[1], rate = variance.params[2])
  for(j in 1:(view1.dim + view2.dim)){
    if(j <= view1.dim){
      for(i in 1:data1.dim){
        generated.matrix[i,j] <- rnorm(1, mean = 0, sd = 1/column.variances[j])
      }
    }
    else if(j > view1.dim){
      for(i in (data1.dim + 1):(data1.dim + data2.dim)){
        generated.matrix[i,j] <- rnorm(1, mean = 0, sd = 1/column.variances[j])
      }
    }
    for(i in 1:(data1.dim + data2.dim)){
      if(i > data1.dim && j <= view1.dim){
        generated.matrix[i,j] = 0
      }
      else if(i <= data1.dim && j > view1.dim){
        generated.matrix[i,j] = 0
      }
    }
  }
  return(generated.matrix)
}

noise.covariance.generator <- function(data1.dim, data2.dim, shared.noise.1, shared.noise.2){
  covariance.noise <- matrix(0, nrow = data1.dim + data2.dim, ncol = data1.dim + data2.dim)
  noise.1 <- shared.noise.1
  noise.2 <- shared.noise.2
  for(i in 1:data1.dim){
    covariance.noise[i,i] <- noise.1
  }
  for(i in (data1.dim + 1):(data1.dim + data2.dim)){
    covariance.noise[i,i] <- noise.2
  }
  return(covariance.noise)
}


CCA.dataset.generator <- function(N.observations, data1.dim, data2.dim, view1.dim, view2.dim, shared.dim, shared.noise.1, shared.noise.2, alpha.eigenvalue, beta.eigenvalue,
                                  column.loading.variance.parameters, percent){
  Y <- matrix(0, nrow = data1.dim + data2.dim, ncol = N.observations)
  prior.generated.eigenvalues <- eigenvalue.simulator(dims = data1.dim + data2.dim, alpha = alpha.eigenvalue, beta = beta.eigenvalue)
  generated.eigenvalues <- eigenvalue.simulator.sparse(eigenvalues = prior.generated.eigenvalues,  percent = percent)
  shared.covariance <- shared.covariance.generator.eigen(generated.eigenvalues)
  noise.covariance <- noise.covariance.generator(data1.dim = data1.dim, data2.dim = data2.dim, shared.noise.1 = shared.noise.1, shared.noise.2 = shared.noise.2)
  view.loading.matrix <- view.specific.matrix.generator(view1.dim = view1.dim, view2.dim = view2.dim, data1.dim = data1.dim, data2.dim = data2.dim, variance.params = column.loading.variance.parameters)
  for(j in 1:N.observations){
    z <- rmvnorm(1, mean = rep(0, view1.dim + view2.dim), sigma = diag(view1.dim + view2.dim))
    Y[,j] <- t(rmvnorm(1, mean = view.loading.matrix%*%t(z), sigma = shared.covariance + noise.covariance))
  }
  generator.list <- list("eigenvalues" = generated.eigenvalues, "shared covariance" = shared.covariance, "noise covariance" = noise.covariance, "view loading matrix" = view.loading.matrix, "generated data" = Y)
  names(generator.list) <- c("eigenvalues", "shared covariance", "noise covariance", "view loading matrix", "generated data")
  return(generator.list)
}


# An example line for generating the CCA dataset:



CCA.dataset.generator(N.observations = 200, data1.dim = 5, data2.dim = 3,
                      view1.dim = 2, view2.dim = 2, shared.dim = 8,
                      shared.noise.1 = .01, shared.noise.2 = .02,
                      alpha.eigenvalue = 1, beta.eigenvalue = 1,
                      column.loading.variance.parameters = c(5,5),
                      percent = .7)


# Here I will begin the task of fitting the model from "Sparse Householder CCA" with Rstan:

library(rstan)

# This will be our first "true" data:


set.seed(1234)
Y <- CCA.dataset.generator(N.observations = 200, data1.dim = 11, data2.dim = 6,
                           view1.dim = 7, view2.dim = 3, shared.dim = 17,
                           shared.noise.1 = .01, shared.noise.2 = .02,
                           alpha.eigenvalue = 1, beta.eigenvalue = 1,
                           column.loading.variance.parameters = c(5,5),
                           percent = .7)
simulation.data <- list(
  N = ncol(Y[[5]]),
  D_1 = 11,
  D_2 = 6,
  K_1 = 7,
  K_2 = 3,
  D = nrow(Y[[5]]),
  Q = 17,
  Y = t(Y[[5]])
)


file.CCA <- "C:/Users/qsimo/Documents/Code/RHouseholder/Sparse householder CCA stuck.stan"



fit.CCA <- stan_model(file.CCA)


fit <- sampling(fit.CCA, data = simulation.data, chains = 1, iter = 10000, cores = parallel::detectCores())


print(fit)



file.CCA.laptop <- "D:/School/Projects/GitMCMCHouseholder/RHouseholder/Sparse householder CCA stuck.stan"

fit.CCA.laptop <- stan_model(file.CCA.laptop)

samples.CCA <- sampling(fit.CCA.laptop, data = simulation.data, chains = 1, iter = 300)

print(samples.CCA)



x <- seq(0,1, by = .01)

plot(x,dbeta(x, shape1 = .01, shape2 = .01), type = "l")


x.2 <- seq(0,4, by = .01)

plot(x.2,dexp(x.2, rate = 3.5), type = "l")
